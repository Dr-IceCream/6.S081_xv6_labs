diff --git a/kernel/bio.c b/kernel/bio.c
index 60d91a6..7795e3b 100644
--- a/kernel/bio.c
+++ b/kernel/bio.c
@@ -23,14 +23,21 @@
 #include "fs.h"
 #include "buf.h"
 
+#define NBUCKET 13
+#define HASH(blockno) (blockno % NBUCKET)
+
 struct {
   struct spinlock lock;
   struct buf buf[NBUF];
 
+  int size;  //record the inserted buf num
+  struct buf buckets[NBUCKET];
+  struct spinlock locks[NBUCKET];
+  struct spinlock hashlock;
   // Linked list of all buffers, through prev/next.
   // Sorted by how recently the buffer was used.
   // head.next is most recent, head.prev is least.
-  struct buf head;
+  //struct buf head;
 } bcache;
 
 void
@@ -39,16 +46,22 @@ binit(void)
   struct buf *b;
 
   initlock(&bcache.lock, "bcache");
+  bcache.size=0;
+  initlock(&bcache.hashlock,"bcache_hash");
+  for(int i=0;i<NBUCKET;i++){
+    initlock(&bcache.locks[i],"bcache_buckets");
+  }
+
 
-  // Create linked list of buffers
+  /*// Create linked list of buffers
   bcache.head.prev = &bcache.head;
-  bcache.head.next = &bcache.head;
+  bcache.head.next = &bcache.head;*/
   for(b = bcache.buf; b < bcache.buf+NBUF; b++){
-    b->next = bcache.head.next;
-    b->prev = &bcache.head;
+    /*b->next = bcache.head.next;
+    b->prev = &bcache.head;*/
     initsleeplock(&b->lock, "buffer");
-    bcache.head.next->prev = b;
-    bcache.head.next = b;
+    /*bcache.head.next->prev = b;
+    bcache.head.next = b;*/
   }
 }
 
@@ -60,20 +73,78 @@ bget(uint dev, uint blockno)
 {
   struct buf *b;
 
-  acquire(&bcache.lock);
+  int index = HASH(blockno);
+  struct buf *pre, *LRUb=0, *LRUpre=0;
+  uint64 mintimestamp;
 
+  acquire(&bcache.locks[index]);
   // Is the block already cached?
-  for(b = bcache.head.next; b != &bcache.head; b = b->next){
+  for(b = bcache.buckets[index].next; b; b = b->next){
     if(b->dev == dev && b->blockno == blockno){
       b->refcnt++;
-      release(&bcache.lock);
+      release(&bcache.locks[index]);
       acquiresleep(&b->lock);
       return b;
     }
   }
-
   // Not cached.
-  // Recycle the least recently used (LRU) unused buffer.
+  acquire(&bcache.lock);
+  if(bcache.size < NBUF){ // there are unused space
+    b = &bcache.buf[bcache.size++];
+    release(&bcache.lock);
+    b->dev=dev;
+    b->blockno=blockno;
+    b->valid=0;
+    b->refcnt=1;
+    b->next=bcache.buckets[index].next;
+    bcache.buckets[index].next=b;
+    release(&bcache.locks[index]);
+    acquiresleep(&b->lock);
+    return b;
+  }
+  release(&bcache.lock);
+  release(&bcache.locks[index]);
+  acquire(&bcache.hashlock);
+  for(int i=0;i<NBUCKET;i++){
+    mintimestamp=-1; // set to maxvalue (because it is unsigned int)
+    acquire(&bcache.locks[index]);
+    pre=&bcache.buckets[index];
+    b=pre->next;
+    while(b){ // find the LRUb
+      if(b->refcnt==0 && b->timestamp<mintimestamp){
+	LRUb=b;
+	LRUpre=pre;
+	mintimestamp=b->timestamp;
+      }
+      pre=b;
+      b=b->next;
+    }
+    if(LRUb){ // have found a valid buf
+      LRUb->dev=dev;
+      LRUb->blockno=blockno;
+      LRUb->valid=0;
+      LRUb->refcnt=1;
+      //if it is in other bucket, move it
+      if(index!=HASH(blockno)){
+	LRUpre->next=LRUb->next;
+	release(&bcache.locks[index]);
+	index=HASH(blockno);
+	acquire(&bcache.locks[index]);
+	LRUb->next=bcache.buckets[index].next;
+	bcache.buckets[index].next=LRUb;
+      }
+      release(&bcache.locks[index]);
+      release(&bcache.hashlock);
+      acquiresleep(&LRUb->lock);
+      return LRUb;
+    }
+    release(&bcache.locks[index]);
+    index++;
+    index = index%NBUCKET;
+  }
+
+        
+  /*// Recycle the least recently used (LRU) unused buffer.
   for(b = bcache.head.prev; b != &bcache.head; b = b->prev){
     if(b->refcnt == 0) {
       b->dev = dev;
@@ -84,7 +155,7 @@ bget(uint dev, uint blockno)
       acquiresleep(&b->lock);
       return b;
     }
-  }
+  }*/
   panic("bget: no buffers");
 }
 
@@ -121,33 +192,37 @@ brelse(struct buf *b)
 
   releasesleep(&b->lock);
 
-  acquire(&bcache.lock);
+  int index = HASH(b->blockno);
+  acquire(&bcache.locks[index]);
   b->refcnt--;
   if (b->refcnt == 0) {
-    // no one is waiting for it.
+    b->timestamp = ticks;
+    /*// no one is waiting for it.
     b->next->prev = b->prev;
     b->prev->next = b->next;
     b->next = bcache.head.next;
     b->prev = &bcache.head;
     bcache.head.next->prev = b;
-    bcache.head.next = b;
+    bcache.head.next = b;*/
   }
   
-  release(&bcache.lock);
+  release(&bcache.locks[index]);
 }
 
 void
 bpin(struct buf *b) {
-  acquire(&bcache.lock);
+  int index=HASH(b->blockno);
+  acquire(&bcache.locks[index]);
   b->refcnt++;
-  release(&bcache.lock);
+  release(&bcache.locks[index]);
 }
 
 void
 bunpin(struct buf *b) {
-  acquire(&bcache.lock);
+  int index=HASH(b->blockno);
+  acquire(&bcache.locks[index]);
   b->refcnt--;
-  release(&bcache.lock);
+  release(&bcache.locks[index]);
 }
 
 
diff --git a/kernel/buf.h b/kernel/buf.h
index 4616e9e..d980166 100644
--- a/kernel/buf.h
+++ b/kernel/buf.h
@@ -5,8 +5,9 @@ struct buf {
   uint blockno;
   struct sleeplock lock;
   uint refcnt;
-  struct buf *prev; // LRU cache list
+  //struct buf *prev; // LRU cache list
   struct buf *next;
   uchar data[BSIZE];
+  uint timestamp; //last using time
 };
 
diff --git a/kernel/kalloc.c b/kernel/kalloc.c
index fa6a0ac..8a59c17 100644
--- a/kernel/kalloc.c
+++ b/kernel/kalloc.c
@@ -21,12 +21,17 @@ struct run {
 struct {
   struct spinlock lock;
   struct run *freelist;
-} kmem;
+  char lockname[10];
+} kmems[NCPU];
 
 void
 kinit()
 {
-  initlock(&kmem.lock, "kmem");
+  for(int i=0;i<NCPU;i++){
+    snprintf(kmems[i].lockname, 10, "kmem_%d", i);
+    initlock(&kmems[i].lock,kmems[i].lockname);
+  }
+  // initlock(&kmem.lock, "kmem");
   freerange(end, (void*)PHYSTOP);
 }
 
@@ -56,10 +61,50 @@ kfree(void *pa)
 
   r = (struct run*)pa;
 
-  acquire(&kmem.lock);
-  r->next = kmem.freelist;
-  kmem.freelist = r;
-  release(&kmem.lock);
+  push_off();
+  int cid=cpuid();
+  pop_off();
+  
+  acquire(&kmems[cid].lock);
+  r->next = kmems[cid].freelist;
+  kmems[cid].freelist = r;
+  release(&kmems[cid].lock);
+}
+
+//try to steal other CPUS' freelist 
+struct run *steal(int cpu_id) {
+  int c = cpu_id ; //from this(next) on to search CPU having freelist
+  struct run *fast, *slow, *head;
+  if(cpu_id != cpuid()) { //the cpu_id in is not correct
+    panic("steal");
+  } 
+  for (int i = 1; i < NCPU; i++) {
+    c++;
+    c = c % NCPU;
+    acquire(&kmems[c].lock);
+    // if the freelist is not null
+    if (kmems[c].freelist) {
+      // spit the freelist  to half
+      slow = head = kmems[c].freelist;
+      fast = slow->next;
+      while (fast) {
+	fast = fast->next;
+	if (fast) {
+	  slow = slow->next;
+	  fast = fast->next;
+	}
+      }
+      kmems[c].freelist = slow->next;
+      release(&kmems[c].lock);
+      slow->next = 0;
+      // return the before half
+      return head;
+    } else {
+      release(&kmems[c].lock);
+    }
+  }
+  // no freelist at all
+  return 0;
 }
 
 // Allocate one 4096-byte page of physical memory.
@@ -70,11 +115,22 @@ kalloc(void)
 {
   struct run *r;
 
-  acquire(&kmem.lock);
-  r = kmem.freelist;
+  push_off();
+  int cid=cpuid();
+  pop_off();
+
+  acquire(&kmems[cid].lock);
+  r = kmems[cid].freelist;
   if(r)
-    kmem.freelist = r->next;
-  release(&kmem.lock);
+    kmems[cid].freelist = r->next;
+
+  if(!r){ //if the freelist is null
+    r=steal(cid); // try to steal other CPU's
+    if(r){ //success to steal
+      kmems[cid].freelist=r->next;
+    }
+  }   
+  release(&kmems[cid].lock);
 
   if(r)
     memset((char*)r, 5, PGSIZE); // fill with junk
diff --git a/time.txt b/time.txt
new file mode 100644
index 0000000..64bb6b7
--- /dev/null
+++ b/time.txt
@@ -0,0 +1 @@
+30
